{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tunable Compression Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "import math\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras \n",
    "from keras import layers\n",
    "from keras import backend as K"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 28, 28, 1)\n",
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset. We use both the training & test MNIST digits.\n",
    "\n",
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "all_digits = np.concatenate([x_train, x_test])\n",
    "all_digits = all_digits.astype(\"float32\") / 255.0\n",
    "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
    "\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_test = np.reshape(x_test, (-1, 28, 28, 1))\n",
    "\n",
    "print(np.shape(all_digits))\n",
    "print(np.shape(x_train))\n",
    "print(np.shape(x_test))\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n",
    "dataset = dataset.shuffle(buffer_size = 1024).batch(batch_size)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 1024).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "test_dataset = test_dataset.shuffle(buffer_size = 1024).batch(batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_86 (Conv2D)          (None, 28, 28, 128)       640       \n",
      "                                                                 \n",
      " batch_normalization_73 (Bat  (None, 28, 28, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_185 (Activation)  (None, 28, 28, 128)      0         \n",
      "                                                                 \n",
      " max_pooling2d_85 (MaxPoolin  (None, 14, 14, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_87 (Conv2D)          (None, 14, 14, 64)        32832     \n",
      "                                                                 \n",
      " activation_186 (Activation)  (None, 14, 14, 64)       0         \n",
      "                                                                 \n",
      " max_pooling2d_86 (MaxPoolin  (None, 7, 7, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_41 (Flatten)        (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 128)               401536    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 435,520\n",
      "Trainable params: 435,264\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_80 (Dense)            (None, 3136)              404544    \n",
      "                                                                 \n",
      " reshape_37 (Reshape)        (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose_101 (Conv2  (None, 7, 7, 64)         16448     \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " activation_187 (Activation)  (None, 7, 7, 64)         0         \n",
      "                                                                 \n",
      " up_sampling2d_70 (UpSamplin  (None, 14, 14, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_102 (Conv2  (None, 14, 14, 128)      32896     \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_74 (Bat  (None, 14, 14, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_188 (Activation)  (None, 14, 14, 128)      0         \n",
      "                                                                 \n",
      " up_sampling2d_71 (UpSamplin  (None, 28, 28, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_transpose_103 (Conv2  (None, 28, 28, 1)        513       \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " activation_189 (Activation)  (None, 28, 28, 1)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 454,913\n",
      "Trainable params: 454,657\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.convolutional.conv2d_transpose import Conv2DTranspose\n",
    "from keras.layers.reshaping.up_sampling2d import UpSampling2D\n",
    "\n",
    "input_shape = (28,28,1)\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "# encoder \n",
    "encoder = keras.Sequential()\n",
    "encoder.add(keras.Input(shape = input_shape))\n",
    "encoder.add(layers.Conv2D(128, (2,2), padding = 'same'))\n",
    "encoder.add(layers.BatchNormalization())\n",
    "encoder.add(layers.Activation('relu'))\n",
    "encoder.add(layers.MaxPooling2D((2,2),padding = 'same'))\n",
    "\n",
    "encoder.add(layers.Conv2D(64, (2,2), activation = 'relu', padding = 'same'))\n",
    "encoder.add(layers.Activation('relu'))\n",
    "encoder.add(layers.MaxPooling2D((2,2),padding = 'same'))\n",
    "\n",
    "encoder.add(layers.Flatten())\n",
    "encoder.add(layers.Dense(latent_dim, activation = 'relu'))\n",
    "\n",
    "encoder.summary()\n",
    "\n",
    "decoder = keras.Sequential()\n",
    "decoder.add(keras.Input(shape = (latent_dim,)))\n",
    "decoder.add(layers.Dense(7*7*64, activation = 'relu'))\n",
    "decoder.add(layers.Reshape(target_shape = (7,7,64)))\n",
    "\n",
    "decoder.add(Conv2DTranspose(64, (2,2), padding = 'same'))\n",
    "decoder.add(layers.Activation('relu'))\n",
    "decoder.add(UpSampling2D((2,2)))\n",
    "\n",
    "decoder.add(Conv2DTranspose(128, (2,2), padding = 'same'))\n",
    "decoder.add(layers.BatchNormalization())\n",
    "decoder.add(layers.Activation('relu'))\n",
    "decoder.add(UpSampling2D((2,2)))\n",
    "\n",
    "decoder.add(Conv2DTranspose(1, (2,2), padding = 'same'))\n",
    "decoder.add(layers.Activation('sigmoid'))\n",
    "\n",
    "\n",
    "decoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.avg_loss_tracker = keras.metrics.Mean(name = 'avg_loss')\n",
    "\n",
    "    def compile(self, optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer \n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "\n",
    "        avg_loss = 0\n",
    "\n",
    "        for i in range(int(np.log2(latent_dim)+1)):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # create mask\n",
    "                ones_length = int(2**(np.log2(latent_dim)-i))\n",
    "                zeros_length = int(latent_dim - ones_length)\n",
    "                ones = tf.ones((ones_length)) \n",
    "                zeros = tf.zeros((zeros_length))\n",
    "                mask = tf.concat([ones, zeros], axis = 0) \n",
    "                mask = tf.reshape(mask, (1,latent_dim))\n",
    "     \n",
    "                codes = self.encoder(data)\n",
    "                masked_codes = tf.multiply(codes, mask)\n",
    "                #print(codes.shape)\n",
    "                #print(mask.shape)\n",
    "                #print(masked_codes.shape)\n",
    "                #masked_codes = tf.reshape(masked_codes, (tf.shape(codes)[0], latent_dim))\n",
    "                \n",
    "                reconstruction = self.decoder(masked_codes)\n",
    "\n",
    "                loss = self.loss_fn(data, reconstruction)\n",
    "            \n",
    "            avg_loss += loss\n",
    "            # get gradients\n",
    "            grads = tape.gradient(loss, self.trainable_weights)\n",
    "            # apply gradients\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        avg_loss = avg_loss / int(np.log2(latent_dim)+1)\n",
    "            \n",
    "        self.avg_loss_tracker.update_state(avg_loss)\n",
    "\n",
    "        return {\n",
    "            'avg_loss': self.avg_loss_tracker.result()\n",
    "        }\n",
    "\n",
    "    def custom_predict(self, data, n):\n",
    "\n",
    "        # create mask\n",
    "        ones_length = int(2**(n))\n",
    "        zeros_length = int(latent_dim - ones_length)\n",
    "        ones = tf.ones((ones_length)) \n",
    "        zeros = tf.zeros((zeros_length))\n",
    "        mask = tf.concat([ones, zeros], axis = 0) \n",
    "        mask = tf.reshape(mask, (1,latent_dim))\n",
    "\n",
    "        codes = self.encoder(data)\n",
    "        masked_codes = tf.multiply(codes, mask)\n",
    "        #masked_codes = tf.reshape(masked_codes, (1, latent_dim))\n",
    "        \n",
    "        reconstruction = self.decoder(masked_codes)\n",
    "\n",
    "        return reconstruction\n",
    "\n",
    "    def test_step(self, data):\n",
    "        \n",
    "        avg_loss = 0\n",
    "\n",
    "        for i in range(int(np.log2(latent_dim)+1)):\n",
    "            # create mask\n",
    "            ones_length = int(2**(np.log2(latent_dim)-i))\n",
    "            zeros_length = int(latent_dim - ones_length)\n",
    "            ones = tf.ones((ones_length)) \n",
    "            zeros = tf.zeros((zeros_length))\n",
    "            mask = tf.concat([ones, zeros], axis = 0) \n",
    "            mask = tf.reshape(mask, (1,latent_dim))\n",
    "\n",
    "            # mask codes\n",
    "            codes = self.encoder(data)\n",
    "            masked_codes = tf.multiply(codes, mask)        \n",
    "            reconstruction = self.decoder(masked_codes)\n",
    "\n",
    "            loss = self.loss_fn(data, reconstruction)\n",
    "            \n",
    "            avg_loss += loss\n",
    "\n",
    "        avg_loss = avg_loss / int(np.log2(latent_dim)+1)\n",
    "            \n",
    "        self.avg_loss_tracker.update_state(avg_loss)\n",
    "\n",
    "        return {\n",
    "            'avg_loss': self.avg_loss_tracker.result()\n",
    "        }\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# test masking \n",
    "\n",
    "i = 5\n",
    "\n",
    "ones_length = int(2**(i))\n",
    "zeros_length = int(latent_dim - ones_length)\n",
    "ones = tf.ones((ones_length)) \n",
    "zeros = tf.zeros((zeros_length))\n",
    "mask = tf.concat([ones, zeros], axis = 0)\n",
    "mask = tf.reshape(mask, (1, latent_dim,))\n",
    "\n",
    "a = mask.numpy()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tcae\n",
    "\n",
    "tcae = TCAE(encoder = encoder, decoder = decoder)\n",
    "tcae.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss_fn = tf.keras.losses.BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 28/469 [>.............................] - ETA: 17:55 - avg_loss: 0.2810"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "tcae.fit(train_dataset, epochs = epochs, validation_data = test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = random.randint(0, np.shape(all_digits)[0]-1)\n",
    "img = all_digits[index,:,:,0]\n",
    "\n",
    "plt.figure(figsize = (3,3))\n",
    "plt.imshow(img)\n",
    "\n",
    "img = img.reshape(1, 28, 28, 1)\n",
    "\n",
    "plt.figure(figsize = (16,5))\n",
    "for i in range(int(np.log2(latent_dim))+1):\n",
    "    ax = plt.subplot(2, 8, i + 1)\n",
    "    reconstruction = tcae.custom_predict([img], np.log2(latent_dim)-i)\n",
    "    reconstruction = reconstruction.numpy().reshape(28,28)\n",
    "    plt.title('latent dim:{}'.format(int(2**(np.log2(latent_dim)-i))))\n",
    "    plt.imshow(reconstruction)\n",
    "    plt.axis('off')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
